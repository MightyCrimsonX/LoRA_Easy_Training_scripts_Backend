{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning AI SDXL LoRA Trainer\n",
        "\n",
        "Este cuaderno prepara un entorno dentro de Lightning AI (ruta ra\u00edz ``/teamspace/studios/this_studio``)\n",
        "y ejecuta el entrenador de LoRA para Stable Diffusion XL incluido en este fork.\n",
        "\n",
        "Los pasos principales son:\n",
        "\n",
        "1. Crear/usar un entorno gestionado por [uv](https://github.com/astral-sh/uv).\n",
        "2. Introducir un nombre de LoRA; el cuaderno crear\u00e1 `/teamspace/studios/this_studio/Loras/<nombre>/` con las subcarpetas\n",
        "   `dataset/` y `output/`. Copia tus im\u00e1genes y `.txt` dentro de `dataset/`. Puedes definir *activation tags* globales y\n",
        "   habilitar el mezclado de tags desde la configuraci\u00f3n.\n",
        "3. Instalar las dependencias necesarias.\n",
        "4. Definir la configuraci\u00f3n de entrenamiento (epochs, repeats, tasas de aprendizaje, tags, modelo base y VAE a descargar autom\u00e1ticamente, etc.).\n",
        "5. Ejecutar el entrenamiento calculando autom\u00e1ticamente los *steps* mediante la f\u00f3rmula requerida.\n",
        "6. Seleccionar optimizadores personalizados (AdamW8bit, Prodigy, DAdapt*, Lion, etc.) y schedulers (`cosine`, `rex`,\n",
        "   `cosine_with_restarts`, entre otros) incluidos en este fork.\n",
        "7. Activa o desactiva los valores recomendados de cada optimizador con `use_optimizer_recommended_args` si quieres aplicar los presets sugeridos (Adafactor, AdamW8bit, Prodigy y CAME) sin tocar manualmente `optimizer_kwargs`.\n",
        "8. Define el nombre final del archivo LoRA con `lora_name` o la bandera `--lora-name` para controlar el `.safetensors` generado.\n",
        "9. Decide si quieres descargar el modelo base en formato Diffusers o como `.safetensors` con `load_diffusers_format` y, para los presets que lo necesiten (como NoobAI), fuerza o desactiva `v_prediction` desde la configuraci\u00f3n o la CLI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "LIGHTNING_ROOT = Path(\"/teamspace/studios/this_studio\")\n",
        "UV_BIN = Path.home() / \".local\" / \"bin\" / \"uv\"\n",
        "\n",
        "if not UV_BIN.exists():\n",
        "    print(\"Instalando uv\u2026\")\n",
        "    subprocess.run(\n",
        "        [\"/bin/bash\", \"-lc\", \"curl -LsSf https://astral.sh/uv/install.sh | sh\"],\n",
        "        check=True,\n",
        "    )\n",
        "else:\n",
        "    print(\"uv ya est\u00e1 instalado\")\n",
        "\n",
        "os.environ[\"PATH\"] = f\"{UV_BIN.parent}:{os.environ['PATH']}\"\n",
        "os.environ.setdefault(\"UV_PROJECT_ENVIRONMENT\", str(LIGHTNING_ROOT / \".venv\"))\n",
        "print(\"Entorno de uv:\", os.environ[\"UV_PROJECT_ENVIRONMENT\"])\n",
        "\n",
        "repo_root = Path.cwd().resolve()\n",
        "if not (repo_root / \"lightning_ai\").exists():\n",
        "    candidate = repo_root / \"LoRA_Easy_Training_scripts_Backend\"\n",
        "    if candidate.exists():\n",
        "        repo_root = candidate.resolve()\n",
        "if not (repo_root / \"lightning_ai\").exists():\n",
        "    raise FileNotFoundError(\"No se encontr\u00f3 la carpeta 'lightning_ai' junto al notebook.\")\n",
        "\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "print(\"Repositorio detectado en:\", repo_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "required_packages = [\n",
        "    \"numpy<2\",\n",
        "    \"bitsandbytes==0.44.0\",\n",
        "    \"prodigyopt\",\n",
        "    \"lion-pytorch==0.0.6\",\n",
        "    \"dadaptation\",\n",
        "    \"pytorch-optimizer==3.1.2\",\n",
        "    \"xformers==0.0.22.post7\",\n",
        "    \"torch>=2.1\",\n",
        "    \"torchvision\",\n",
        "    \"accelerate>=0.23\",\n",
        "    \"diffusers[torch]==0.25\",\n",
        "    \"transformers==4.35\",\n",
        "    \"peft>=0.7\",\n",
        "    \"safetensors==0.4.4\",\n",
        "    \"huggingface-hub==0.24.5\",\n",
        "    \"toml==0.10.2\",\n",
        "    \"tensorboard\",\n",
        "    \"pytorch-lightning==1.9.0\",\n",
        "    \"opencv-python==4.8.1.78\",\n",
        "]\n",
        "print(\"Instalando dependencias con uv\u2026\")\n",
        "subprocess.run([\"uv\", \"pip\", \"install\", \"--upgrade\", *required_packages], check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from lightning_ai.sdxl_lora_trainer import (\n",
        "    BASE_MODEL_PRESETS,\n",
        "    VAE_PRESETS,\n",
        "    TrainingConfig,\n",
        ")\n",
        "\n",
        "lora_name = \"mi_lora_personalizado\"\n",
        "\n",
        "lora_workspace = (LIGHTNING_ROOT / \"Loras\" / lora_name).resolve()\n",
        "dataset_workspace = lora_workspace / \"dataset\"\n",
        "output_workspace = lora_workspace / \"output\"\n",
        "dataset_workspace.mkdir(parents=True, exist_ok=True)\n",
        "output_workspace.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "base_config = TrainingConfig(\n",
        "    dataset_dir=dataset_workspace,  # carpeta con im\u00e1genes y .txt emparejados\n",
        "    output_dir=output_workspace,\n",
        "    num_epochs=2,\n",
        "    batch_size=1,\n",
        "    gradient_accumulation=1,\n",
        "    num_repeats=2,\n",
        "    resolution=1024,\n",
        "    network_rank=64,\n",
        "    network_alpha=128,\n",
        "    base_model=\"Stable Diffusion XL 1.0 base\",\n",
        "    load_diffusers_format=True,  # cambia a False para descargar checkpoints .safetensors\n",
        "    v_prediction=None,  # usa True/False para forzar o desactivar v-prediction manualmente\n",
        "    vae_model=\"Made by Ollin SDXL VAE FP16 Fix\",\n",
        "    unet_lr=1e-4,\n",
        "    text_encoder_lr=5e-6,\n",
        "    optimizer_type=\"adamw\",\n",
        "    weight_decay=1e-2,\n",
        "    optimizer_beta1=0.9,\n",
        "    optimizer_beta2=0.999,\n",
        "    optimizer_eps=1e-8,\n",
        "    optimizer_momentum=0.9,\n",
        "    scheduler_type=\"cosine\",\n",
        "    lr_warmup_steps=None,\n",
        "    scheduler_first_cycle_steps=None,\n",
        "    scheduler_cycle_multiplier=1.0,\n",
        "    scheduler_gamma=1.0,\n",
        "    scheduler_min_lr=1e-6,\n",
        "    scheduler_d=0.9,\n",
        "    scheduler_power=1.0,\n",
        "    train_text_encoders=True,\n",
        "    mixed_precision=\"fp16\",\n",
        "    shuffle_tags=True,\n",
        "    activation_tags=(\"mi_lora\",),\n",
        "    use_optimizer_recommended_args=True,\n",
        "    cross_attention_backend=\"xformers\",\n",
        "    lora_name=lora_name,  # la extensi\u00f3n .safetensors se a\u00f1ade si falta\n",
        ")\n",
        "\n",
        "print(\"Directorio de trabajo del LoRA:\", lora_workspace)\n",
        "print(\"Modelos base disponibles (Diffusers / Safetensors):\")\n",
        "for name, preset in BASE_MODEL_PRESETS.items():\n",
        "    print(f\"  - {name}: diffusers={preset.diffusers_id} | safetensors={preset.single_file_url}\")\n",
        "\n",
        "print(\"VAEs disponibles (Diffusers / Safetensors):\")\n",
        "for name, preset in VAE_PRESETS.items():\n",
        "    print(f\"  - {name}: diffusers={preset.diffusers_id} | safetensors={preset.single_file_url}\")\n",
        "\n",
        "config = base_config.normalised_paths()\n",
        "print(\"Configuraci\u00f3n resuelta:\")\n",
        "for key, value in config.__dict__.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from lightning_ai.sdxl_lora_trainer import FolderCaptionDataset, calculate_total_steps\n",
        "\n",
        "preview_dataset = FolderCaptionDataset(\n",
        "    dataset_dir=config.dataset_dir,\n",
        "    resolution=config.resolution,\n",
        "    activation_tags=config.activation_tags,\n",
        "    shuffle_tags=config.shuffle_tags,\n",
        ")\n",
        "\n",
        "num_images = len(preview_dataset)\n",
        "steps = calculate_total_steps(\n",
        "    num_images=num_images,\n",
        "    num_repeats=config.num_repeats,\n",
        "    num_epochs=config.num_epochs,\n",
        "    batch_size=config.batch_size,\n",
        ")\n",
        "print(json.dumps(\n",
        "    {\n",
        "        \"num_images\": num_images,\n",
        "        \"num_repeats\": config.num_repeats,\n",
        "        \"num_epochs\": config.num_epochs,\n",
        "        \"batch_size\": config.batch_size,\n",
        "        \"calculated_steps\": steps,\n",
        "    },\n",
        "    indent=2,\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from lightning_ai.sdxl_lora_trainer import train\n\nprint(\"Iniciando entrenamiento\u2026\")\ntrain(config)"
    }
  ]
}