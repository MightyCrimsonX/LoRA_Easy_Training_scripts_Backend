{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning AI SDXL LoRA Trainer (kohya)\n\n",
    "Este cuaderno prepara un entorno dentro de Lightning AI (ruta raíz ``/teamspace/studios/this_studio``)\n",
    "y utiliza la integración reciente con [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts).\n",
    "Los pasos principales son:\n\n",
    "1. Instalar/usar un entorno gestionado por [uv](https://github.com/astral-sh/uv).\n",
    "2. Detectar el repositorio backend y exponer el módulo `sd_scripts` con las utilidades de `utils.kohya`.\n",
    "3. Instalar las dependencias necesarias (backend, kohya y extras de este fork).\n",
    "4. Configurar las rutas de trabajo del LoRA y definir los parámetros básicos del entrenamiento.\n",
    "5. Generar los ficheros `config.toml` y `dataset.toml` con `utils.process`.\n",
    "6. Construir el comando de entrenamiento con `build_training_command` y, opcionalmente, lanzarlo desde el cuaderno.\n\n",
    "Completa los valores marcados como ``<...>`` antes de ejecutar el entrenamiento real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "LIGHTNING_ROOT = Path(\"/teamspace/studios/this_studio\")\n",
    "UV_BIN = Path.home() / \".local\" / \"bin\" / \"uv\"\n",
    "\n",
    "if not UV_BIN.exists():\n",
    "    print(\"Instalando uv…\")\n",
    "    subprocess.run(\n",
    "        [\"/bin/bash\", \"-lc\", \"curl -LsSf https://astral.sh/uv/install.sh | sh\"],\n",
    "        check=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"uv ya está instalado\")\n",
    "\n",
    "os.environ[\"PATH\"] = f\"{UV_BIN.parent}:{os.environ['PATH']}\"\n",
    "os.environ.setdefault(\"UV_PROJECT_ENVIRONMENT\", str(LIGHTNING_ROOT / \".venv\"))\n",
    "print(\"Entorno de uv:\", os.environ[\"UV_PROJECT_ENVIRONMENT\"])\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "if not (project_root / \"utils\").exists():\n",
    "    candidate = project_root / \"LoRA_Easy_Training_scripts_Backend\"\n",
    "    if candidate.exists():\n",
    "        project_root = candidate.resolve()\n",
    "if not (project_root / \"utils\").exists():\n",
    "    raise FileNotFoundError(\"No se encontró la carpeta 'utils' junto al notebook.\")\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "print(\"Repositorio detectado en:\", project_root)\n",
    "\n",
    "from utils.kohya import ensure_on_path, get_repo_root\n",
    "\n",
    "ensure_on_path()\n",
    "kohya_root = get_repo_root()\n",
    "print(\"Repositorio kohya:\", kohya_root)\n",
    "\n",
    "runtime_store = project_root / \"runtime_store\"\n",
    "runtime_store.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Carpeta runtime_store lista en:\", runtime_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import subprocess\n",
    "\n",
    "    commands: list[list[str]] = []\n",
    "\n",
    "    base_requirements = project_root / \"requirements.txt\"\n",
    "    if base_requirements.exists():\n",
    "        commands.append([\"uv\", \"pip\", \"install\", \"--upgrade\", \"-r\", str(base_requirements)])\n",
    "\n",
    "    for local_pkg in (\"custom_scheduler\", \"lycoris\"):\n",
    "        pkg_path = project_root / local_pkg\n",
    "        if pkg_path.exists():\n",
    "            commands.append([\"uv\", \"pip\", \"install\", \"--upgrade\", str(pkg_path)])\n",
    "\n",
    "    kohya_requirements = kohya_root / \"requirements.txt\"\n",
    "    if kohya_requirements.exists():\n",
    "        commands.append([\"uv\", \"pip\", \"install\", \"--upgrade\", \"-r\", str(kohya_requirements)])\n",
    "    else:\n",
    "        print(\"No se encontró requirements.txt dentro de sd_scripts; se omite la instalación específica.\")\n",
    "\n",
    "    for command in commands:\n",
    "        print(\"Ejecutando:\", \" \".join(command))\n",
    "        subprocess.run(command, check=True)\n",
    "\n",
    "    accelerate_config = LIGHTNING_ROOT / \".cache\" / \"huggingface\" / \"accelerate\" / \"default_config.yaml\"\n",
    "    if not accelerate_config.exists():\n",
    "        accelerate_config.parent.mkdir(parents=True, exist_ok=True)\n",
    "        accelerate_config.write_text(\n",
    "            \"command_file: null\n",
    "\"\n",
    "            \"commands: null\n",
    "\"\n",
    "            \"compute_environment: LOCAL_MACHINE\n",
    "\"\n",
    "            \"deepspeed_config: {}\n",
    "\"\n",
    "            \"distributed_type: 'NO'\n",
    "\"\n",
    "            \"downcase_fp16: 'NO'\n",
    "\"\n",
    "            \"dynamo_backend: 'NO'\n",
    "\"\n",
    "            \"fsdp_config: {}\n",
    "\"\n",
    "            \"gpu_ids: '0'\n",
    "\"\n",
    "            \"machine_rank: 0\n",
    "\"\n",
    "            \"main_process_ip: null\n",
    "\"\n",
    "            \"main_process_port: null\n",
    "\"\n",
    "            \"main_training_function: main\n",
    "\"\n",
    "            \"megatron_lm_config: {}\n",
    "\"\n",
    "            \"mixed_precision: bf16\n",
    "\"\n",
    "            \"num_machines: 1\n",
    "\"\n",
    "            \"num_processes: 1\n",
    "\"\n",
    "            \"rdzv_backend: static\n",
    "\"\n",
    "            \"same_network: true\n",
    "\"\n",
    "            \"tpu_name: null\n",
    "\"\n",
    "            \"tpu_zone: null\n",
    "\"\n",
    "            \"use_cpu: false\"\n",
    "        )\n",
    "        print(\"Archivo de configuración de accelerate creado en:\", accelerate_config)\n",
    "    else:\n",
    "        print(\"Configuración de accelerate existente en:\", accelerate_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from utils.process import process_args, process_dataset_args\n",
    "from lightning_ai.sdxl_lora_trainer import (\n",
    "    BASE_MODEL_NAMES,\n",
    "    BASE_MODEL_PRESETS,\n",
    "    MODEL_CACHE_ROOT,\n",
    "    VAE_MODEL_NAMES,\n",
    "    VAE_PRESETS,\n",
    "    ensure_model_assets,\n",
    "    ensure_model_file,\n",
    ")\n",
    "\n",
    "lora_name = \"mi_lora_kohya\"\n",
    "workspace_root = (LIGHTNING_ROOT / \"Loras\" / lora_name).resolve()\n",
    "dataset_workspace = workspace_root / \"dataset\"\n",
    "output_workspace = workspace_root / \"output\"\n",
    "base_model_dir = (MODEL_CACHE_ROOT / \"base\").resolve()\n",
    "vae_dir = (MODEL_CACHE_ROOT / \"vae\").resolve()\n",
    "for path in (workspace_root, dataset_workspace, output_workspace, base_model_dir, vae_dir):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directorio de trabajo del LoRA:\", workspace_root)\n",
    "print(\"Carpeta del dataset:\", dataset_workspace)\n",
    "print(\"Carpeta de salida:\", output_workspace)\n",
    "print(\"Carpeta local para modelos base:\", base_model_dir)\n",
    "print(\"Carpeta local para VAEs:\", vae_dir)\n",
    "\n",
    "load_diffusers = True\n",
    "training_model = \"Pony Diffusion V6 XL\"\n",
    "vae_choice = \"Stability AI SDXL VAE\"\n",
    "custom_base_model_path = \"\"\n",
    "custom_vae_path = \"\"\n",
    "\n",
    "if training_model and training_model not in BASE_MODEL_NAMES:\n",
    "    raise ValueError(\n",
    "        f\"Modelo base desconocido '{training_model}'. Opciones válidas: {', '.join(BASE_MODEL_NAMES)}\"\n",
    "    )\n",
    "if vae_choice and vae_choice not in VAE_MODEL_NAMES:\n",
    "    raise ValueError(\n",
    "        f\"VAE desconocido '{vae_choice}'. Opciones válidas: {', '.join(VAE_MODEL_NAMES)}\"\n",
    "    )\n",
    "\n",
    "if custom_base_model_path:\n",
    "    base_target = Path(custom_base_model_path).expanduser().resolve()\n",
    "    print(\"Usando modelo base personalizado en:\", base_target)\n",
    "else:\n",
    "    preset = BASE_MODEL_PRESETS[training_model]\n",
    "    if load_diffusers:\n",
    "        base_target = ensure_model_assets(preset.diffusers_id, \"base\", training_model)\n",
    "    else:\n",
    "        base_target = ensure_model_file(\n",
    "            preset.single_file_url,\n",
    "            \"base\",\n",
    "            training_model,\n",
    "            preset.single_file_name,\n",
    "        )\n",
    "    print(\"Modelo base listo en:\", base_target)\n",
    "\n",
    "if custom_vae_path:\n",
    "    vae_target = Path(custom_vae_path).expanduser().resolve()\n",
    "    print(\"Usando VAE personalizado en:\", vae_target)\n",
    "elif vae_choice:\n",
    "    vae_preset = VAE_PRESETS[vae_choice]\n",
    "    if load_diffusers:\n",
    "        vae_target = ensure_model_assets(vae_preset.diffusers_id, \"vae\", vae_choice)\n",
    "    else:\n",
    "        vae_target = ensure_model_file(\n",
    "            vae_preset.single_file_url,\n",
    "            \"vae\",\n",
    "            vae_choice,\n",
    "            vae_preset.single_file_name,\n",
    "        )\n",
    "    print(\"VAE listo en:\", vae_target)\n",
    "else:\n",
    "    vae_target = None\n",
    "    print(\"No se utilizará VAE adicional.\")\n",
    "\n",
    "base_v_prediction = False\n",
    "if not custom_base_model_path and training_model:\n",
    "    base_v_prediction = BASE_MODEL_PRESETS[training_model].default_vpred\n",
    "\n",
    "print(\"Predicción V activada:\", base_v_prediction)\n",
    "\n",
    "\n",
    "def _stringify(target):\n",
    "    if target is None:\n",
    "        return None\n",
    "    return target if isinstance(target, str) else target.as_posix()\n",
    "\n",
    "config_args = {\n",
    "    \"pretrained_model_name_or_path\": _stringify(base_target),\n",
    "    \"vae\": _stringify(vae_target),\n",
    "    \"output_dir\": output_workspace.as_posix(),\n",
    "    \"logging_dir\": (output_workspace / \"logs\").as_posix(),\n",
    "    \"network_module\": \"networks.lora\",\n",
    "    \"network_dim\": 32,\n",
    "    \"network_alpha\": 32,\n",
    "    \"train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_train_epochs\": 1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"unet_lr\": 1e-4,\n",
    "    \"text_encoder_lr\": 5e-6,\n",
    "    \"optimizer_type\": \"AdamW8bit\",\n",
    "    \"save_every_n_steps\": 200,\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"cache_latents\": True,\n",
    "    \"clip_skip\": 2,\n",
    "    \"seed\": 42,\n",
    "    \"load_diffusers_format\": load_diffusers,\n",
    "    \"v_prediction\": base_v_prediction,\n",
    "}\n",
    "\n",
    "dataset_args = {\n",
    "    \"general\": {\n",
    "        \"resolution\": 1024,\n",
    "        \"shuffle_caption\": True,\n",
    "        \"caption_extension\": \".txt\",\n",
    "        \"keep_tokens\": 1,\n",
    "    },\n",
    "    \"subsets\": [\n",
    "        {\n",
    "            \"image_dir\": dataset_workspace.as_posix(),\n",
    "            \"caption_dropout_rate\": 0.0,\n",
    "            \"caption_dropout_every_n_epochs\": 0,\n",
    "            \"num_repeats\": 1,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "config_lines, config_path = process_args(config_args)\n",
    "dataset_lines, dataset_path = process_dataset_args(dataset_args)\n",
    "\n",
    "print(\"config.toml generado en:\", config_path)\n",
    "print(\"dataset.toml generado en:\", dataset_path)\n",
    "print(\"Entradas de config.toml:\")\n",
    "for line in config_lines:\n",
    "    print(\"  \", line)\n",
    "print(\"Secciones de dataset.toml:\")\n",
    "for line in dataset_lines.get(\"general\", []):\n",
    "    print(\"  \", line)\n",
    "for index, subset in enumerate(dataset_lines.get(\"subsets\", []), start=1):\n",
    "    print(f\"  [[datasets.subsets]] #{index}\")\n",
    "    for value in subset:\n",
    "        print(\"    \", value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "\n",
    "from utils.kohya import (\n",
    "    build_training_command,\n",
    "    get_training_script,\n",
    "    inherit_environment,\n",
    "    normalise_extra_args,\n",
    ")\n",
    "\n",
    "train_mode = \"lora\"\n",
    "is_sdxl = True\n",
    "is_flux = False\n",
    "extra_cli_arguments = \"--min_snr_gamma=5.0\"\n",
    "use_accelerate = True\n",
    "\n",
    "script_path = get_training_script(train_mode, is_sdxl, is_flux)\n",
    "command = build_training_command(\n",
    "    sys.executable,\n",
    "    script_path,\n",
    "    config_path.resolve(),\n",
    "    dataset_path.resolve(),\n",
    "    normalise_extra_args(extra_cli_arguments),\n",
    "    use_accelerate,\n",
    ")\n",
    "\n",
    "print(\"Script seleccionado:\", script_path.name)\n",
    "print(\"Comando completo:\")\n",
    "print(shlex.join(command))\n",
    "\n",
    "run_training = False\n",
    "if run_training:\n",
    "    completed = subprocess.run(\n",
    "        command,\n",
    "        cwd=str(script_path.parent),\n",
    "        env=inherit_environment(),\n",
    "        check=True,\n",
    "    )\n",
    "    print(\"Proceso finalizado con código:\", completed.returncode)\n",
    "else:\n",
    "    print(\"Define `run_training = True` para lanzar el entrenamiento una vez revisados los parámetros.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}