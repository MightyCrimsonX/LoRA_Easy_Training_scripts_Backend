{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omCLgUOB2_ZF"
   },
   "source": [
    "# üåü XL LoRA Trainer (Lightning.ai) por Mighty Crimson\n",
    "\n",
    "Este notebook adapta el flujo de trabajo del proyecto original de [hollowstrawberry](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) y el fork de [whitez](https://colab.research.google.com/github/gwhitez/Lora-Trainer-XL/blob/main/Fix_Lora_Trainer_XL.ipynb) usando [Kohya](https://github.com/kohya-ss/sd-scripts/tree/5a18a03ffcc2a21c6e884a25d041076911a79a2a) a para ejecutarse dentro de Lightning.ai.<br>\n",
    "Si se les hace compicado entender todo este notebook, comienzen primero haciendo loras en colab para familiarizarse con este notebook.<br>\n",
    "Recomendable solo usar la grafica L4, ya que es suficiente para todo.<br>\n",
    "Todo el trabajo se realiza en el directorio base `/teamspace/studios/this_studio`.\n",
    "\n",
    "> Basado en el trabajo de [Kohya-ss](https://github.com/kohya-ss/sd-scripts), [Linaqruf](https://github.com/Linaqruf/kohya-trainer) y los colaboradores de este fork.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ8clWTZEu-g"
   },
   "source": [
    "### ‚≠ï Aviso\n",
    "Este cuaderno est√° pensado para investigaci√≥n y entrenamiento de modelos LoRA de forma responsable.\n",
    "Aseg√∫rate de respetar los t√©rminos de servicio de Lightning.ai y de cualquier repositorio o dataset que utilices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPQlB4djNm3C"
   },
   "source": [
    "| Recurso | Enlace |\n",
    "| :--- | :--- |\n",
    "| C√≥digo base del entrenador | [gwhitez/LoRA_Easy_Training_scripts_Backend](https://github.com/gwhitez/LoRA_Easy_Training_scripts_Backend) |\n",
    "| Scripts originales | [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) |\n",
    "| Adaptaci√≥n Lightning | Este notebook |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style='color: yellow;'>Dependencias Instaladas, Reinicia el Kernel</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Dependencias, solo inicia una vez esta celda y reinicia el kernel\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "!git clone https://github.com/MightyCrimsonX/LoRA_Easy_Training_scripts_Backend.git\n",
    "\n",
    "packages = [\n",
    "    \"accelerate==0.33.0\",\n",
    "    \"xformers\",\n",
    "    \"transformers==4.44.0\",\n",
    "    \"diffusers==0.25.0\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"ftfy==6.1.1\",\n",
    "    \"opencv-python==4.8.1.78\",\n",
    "    \"einops==0.7.0\",\n",
    "    \"pytorch-lightning==1.9.0\",\n",
    "    \"bitsandbytes==0.48.2\",\n",
    "    \"lion-pytorch==0.0.6\",\n",
    "    \"schedulefree==1.4\",\n",
    "    \"prodigy-plus-schedule-free==1.9.0\",\n",
    "    \"prodigyopt==1.1.2\",\n",
    "    \"tensorboard\",\n",
    "    \"safetensors==0.4.4\",\n",
    "    \"altair==4.2.2\",\n",
    "    \"easygui==0.98.3\",\n",
    "    \"toml==0.10.2\",\n",
    "    \"voluptuous==0.13.1\",\n",
    "    \"huggingface-hub==0.24.5\",\n",
    "    \"imagesize==1.4.1\",\n",
    "    \"numpy<=2.0\",\n",
    "    \"rich==13.7.0\",\n",
    "    \"sentencepiece==0.2.0\",\n",
    "    \"tarlette\",\n",
    "    \"uvicorn[standard]\",\n",
    "    \"requests\",\n",
    "    \"dadaptation\",\n",
    "    \"wandb\",\n",
    "    \"pyngrok\",\n",
    "    \"pycloudflared\",\n",
    "    \"scipy\",\n",
    "    \"came-pytorch\",\n",
    "    \"pytorch_optimizer==3.1.2\",\n",
    "    \"wheel\"\n",
    "]\n",
    "\n",
    "# Instalar cada paquete individualmente\n",
    "for package in packages:\n",
    "    !pip install {package}\n",
    "!sudo apt install aria2 -q\n",
    "%cd /teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend\n",
    "!git clone https://github.com/kohya-ss/sd-scripts.git\n",
    "clear_output()\n",
    "display(HTML(f\"<h1 style='color: yellow;'>Dependencias Instaladas, Reinicia el Kernel</h1>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui creas la carpeta de tu lora a entrenar en \"project_name\"\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "base_path = Path(\"/teamspace/studios/this_studio\")\n",
    "loras_path = base_path / \"lora_projects\"\n",
    "project_name = \" \"\n",
    "project_path = loras_path / project_name\n",
    "dataset_path = project_path / \"dataset\"\n",
    "\n",
    "loras_path.mkdir(parents=True, exist_ok=True)\n",
    "project_path.mkdir(exist_ok=True)\n",
    "dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "display(HTML(f\"<h1 style='color: yellow;'>Carpetas creadas, coloca tus imagenes y .txt en la carpeta dataset</h1>\"))\n",
    "display(HTML(f\"<h1 style='color: yellow;'>No te olvides de tambien configurar tus presets y poner tambien el mismo nombre del lora en la celda de abajo!</h1>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AT1WRbhiHRCJ"
   },
   "outputs": [],
   "source": [
    "#lEE todos los parametros y modificalos segun tu entrenamiento\n",
    "#pon el mismo nombre del lora que usaste en la celda de arriba, en el parametro project_name\n",
    "#una vez todo hecho y subido tu dataset, solo inicia esta celda y se ejecutar√° el entrenador.\n",
    "import os, re, sys, toml\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import time\n",
    "from IPython.display import Markdown, display, HTML, clear_output\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "import logging\n",
    "\n",
    "root_dir = \"/teamspace/studios/this_studio\"\n",
    "trainer_dir = os.path.join(root_dir, \"LoRA_Easy_Training_scripts_Backend\")\n",
    "kohya_dir = os.path.join(trainer_dir, \"sd-scripts\")\n",
    "models_dir = \"/teamspace/studios/this_studio/models\"\n",
    "downloads_dir = os.path.join(root_dir, \"downloads\")\n",
    "custom_optimizer_path = os.path.join(trainer_dir, \"custom_scheduler\")\n",
    "if custom_optimizer_path not in sys.path:\n",
    "  sys.path.append(custom_optimizer_path)\n",
    "os.environ[\"PYTHONPATH\"] = custom_optimizer_path + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "# Lightning notebooks run continuously; automatic shutdown is not managed here.\n",
    "print(\"üîµ Lightning environment detectado. Det√©n el cuaderno manualmente cuando termines.\")\n",
    "\n",
    "# These carry information from past executions\n",
    "if \"model_url\" in globals():\n",
    "  old_model_url = model_url\n",
    "else:\n",
    "  old_model_url = None\n",
    "if \"dependencies_installed\" not in globals():\n",
    "  dependencies_installed = False\n",
    "if \"model_file\" not in globals():\n",
    "  model_file = None\n",
    "\n",
    "# These may be set by other cells, some are legacy\n",
    "if \"custom_dataset\" not in globals():\n",
    "  custom_dataset = None\n",
    "if \"override_dataset_config_file\" not in globals():\n",
    "  override_dataset_config_file = None\n",
    "if \"override_config_file\" not in globals():\n",
    "  override_config_file = None\n",
    "\n",
    "COMMIT = \"fa2427c6b468231e8e270e40fe72add780118dbe\"\n",
    "LOWRAM = False\n",
    "LOAD_TRUNCATED_IMAGES = True\n",
    "BETTER_EPOCH_NAMES = True\n",
    "FIX_DIFFUSERS = True\n",
    "FIX_WANDB_WARNING = True\n",
    "\n",
    "#@title ## üö© Start Here\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Setup\n",
    "#@markdown El nombre de tu proyecto ser√° el mismo que el de la carpeta que contiene tus im√°genes. No se permiten espacios, puedes usar `gui√≥n bajo` si el nombre es muy largo.\n",
    "project_name = \" \" #@param {type:\"string\"}\n",
    "project_name = project_name.strip()\n",
    "#@markdown La estructura de carpetas no importa y es puramente por comodidad. Aseg√∫rate de elegir siempre el mismo.  Me gusta organizar por proyecto.\n",
    "folder_structure = \"Organize by project (lora_projects/project_name/dataset)\" #@param [\"Organize by category (lora_training/datasets/project_name)\", \"Organize by project (lora_projects/project_name/dataset)\"]\n",
    "#@markdown Decida el modelo que se descargar√° y utilizar√° para el entrenamiento. Tambi√©n puedes elegir tu propio modelo pegando su enlace de descarga o proporcionando una ruta dentro de `/teamspace/studios/this_studio`.\n",
    "training_model = \"Illustrious_2.0\" # @param [\"Pony Diffusion V6 XL\",\"Animagine XL V3\",\"animagine_4.0_zero\",\"Illustrious_0.1\",\"Illustrious_2.0\",\"NoobAI-XL0.75\",\"NoobAI-XL0.5\",\"Stable Diffusion XL 1.0 base\",\"NoobAIXL0_75vpred\",\"RouWei_v080vpred\"]\n",
    "optional_custom_training_model = \"\" #@param {type:\"string\"}\n",
    "#@markdown Esto forzara el uso del modelo en formato diffusers, puede ser util en ciertos casos. <p>\n",
    "#@markdown Manten esto desmarcado para usar un modelo ckpt (.safetensors) para el entrenamiento.\n",
    "force_load_diffusers = False # @param {\"type\":\"boolean\"}\n",
    "#@markdown Marca est√° opci√≥n si el modelo custom esta en dicho formato\n",
    "custom_model_is_diffusers = False #@param {type:\"boolean\"}\n",
    "#@markdown Marca esta opci√≥n si tu modelo soporta vpred de lo contrario dejala desmarcada.\n",
    "custom_model_is_vpred = False #@param {type:\"boolean\"}\n",
    "#@markdown Utilice wandb si desea visualizar el progreso de su entrenamiento a lo largo del tiempo.\n",
    "wandb_key = \"\" #@param {type:\"string\"}\n",
    "\n",
    "load_diffusers = (custom_model_is_diffusers and len(optional_custom_training_model) > 0) \\\n",
    "                 or force_load_diffusers\n",
    "vpred = custom_model_is_vpred and len(optional_custom_training_model) > 0\n",
    "\n",
    "if optional_custom_training_model:\n",
    "  model_url = optional_custom_training_model\n",
    "elif \"Pony\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Pony_diffusion_v6_diffusers_fp16\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/PonyXL/resolve/main/PonyDiffusionV6XL.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"ponyDiffusionV6XL.safetensors\")\n",
    "elif \"Animagine\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-3.0\"\n",
    "  else:\n",
    "    model_url = \"https://civitai.com/api/download/models/293564\"\n",
    "  model_file = os.path.join(models_dir, \"animagineXLV3.safetensors\")\n",
    "elif \"animagine_4.0_zero\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/cagliostrolab/animagine-xl-4.0-zero/resolve/main/animagine-xl-4.0-zero.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"animagine-xl-4.0-zero.safetensors\")\n",
    "elif \"Illustrious_0.1\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/OnomaAIResearch/Illustrious-xl-early-release-v0/resolve/main/Illustrious-XL-v0.1.safetensors\"\n",
    "elif \"Illustrious_2.0\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/Illustrious_2.0/resolve/main/illustriousXL20_v20.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"illustriousXL20_v20.safetensors\")\n",
    "elif \"NoobAI-XL0.75\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.75/resolve/main/NoobAI-XL-v0.75.safetensors\"\n",
    "elif \"NoobAI-XL0.5\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-0.5/resolve/main/NoobAI-XL-v0.5.safetensors\"\n",
    "elif \"Stable Diffusion XL 1.0 base\" in training_model:\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
    "elif \"NoobAIXL0_75vpred\" in training_model:\n",
    "  vpred = True\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/Laxhar/noobai-XL-Vpred-0.75/resolve/main/NoobAI-XL-Vpred-v0.75.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"NoobAI-XL-Vpred-v0.75.safetensors\")\n",
    "else:\n",
    "  vpred = True\n",
    "  if load_diffusers:\n",
    "    model_url = \"https://huggingface.co/John6666/rouwei-v080-vpred-sdxl\"\n",
    "  else:\n",
    "    model_url = \"https://huggingface.co/WhiteAiZ/RouWei/resolve/main/rouwei_v080Vpred.safetensors\"\n",
    "  model_file = os.path.join(models_dir, \"rouwei_v080Vpred.safetensors\")\n",
    "\n",
    "if load_diffusers:\n",
    "  vae_file= \"stabilityai/sdxl-vae\"\n",
    "else:\n",
    "  vae_url = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\"\n",
    "  vae_file = os.path.join(models_dir, \"sdxl_vae.safetensors\")\n",
    "\n",
    "model_url = model_url.strip()\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Processing\n",
    "#@markdown Por defecto la resoluci√≥n para personajes es 1024. otras resoluciones que puedes usar son 896 (recomendado para personajes o 1024) y 768 (recomendado para estilos, puedes usar m√°s repeticiones con esta resoluci√≥n).\n",
    "resolution = 1024 #@param {type:\"dropdown\", min:768, max:1536, step:128}\n",
    "#@markdown Activa `Flip Aug`si tu dataset es peque√±o, util en personajes isometricos, volteara todas tus imagenes (modo espejo) para aprender el doble, pero podria afectar a personajes con tatuajes, marcas, cicatrices etc...\n",
    "flip_aug = False #@param {type:\"boolean\"}\n",
    "caption_extension = \".txt\" # @param [\".txt\",\".caption\"]\n",
    "#@markdown Mezcla etiquetas de anime, mejora el aprendizaje y las indicaciones.  Una etiqueta de activaci√≥n va al comienzo de cada archivo de texto y no se mezclar√°.<p>\n",
    "shuffle_tags = True #@param {type:\"boolean\"}\n",
    "shuffle_caption = shuffle_tags\n",
    "activation_tags = \"1\" #@param [0,1,2,3]\n",
    "keep_tokens = int(activation_tags)\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Steps <p>\n",
    "#@markdown Tus im√°genes se repetir√°n esta cantidad de veces durante el entrenamiento. Te recomiendo que tus im√°genes multiplicadas por sus repeticiones est√© entre 200 y 400.\n",
    "num_repeats = 2 #@param {type:\"number\"}\n",
    "#@markdown Elige cu√°nto tiempo quieres entrenar.  Un buen punto de partida es alrededor de 10 √©pocas o alrededor de 2000 pasos.<p>\n",
    "#@markdown Una √©poca es una cantidad de pasos igual a: la cantidad de im√°genes multiplicada por sus repeticiones, dividida por el tama√±o del lote. <p>\n",
    "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
    "how_many = 40 #@param {type:\"number\"}\n",
    "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
    "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
    "#@markdown Guardar m√°s √©pocas te permitir√° comparar mejor el progreso de tu Lora.\n",
    "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
    "keep_only_last_n_epochs = 5 #@param {type:\"number\"}\n",
    "if not save_every_n_epochs:\n",
    "  save_every_n_epochs = max_train_epochs\n",
    "if not keep_only_last_n_epochs:\n",
    "  keep_only_last_n_epochs = max_train_epochs\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Learning\n",
    "#@markdown La tasa de aprendizaje es lo m√°s importante para tus resultados. Si quieres entrenar m√°s lento con muchas im√°genes, o si tu dim y alfa son altos, mueve el unet a 2e-4 o menos.  <p>\n",
    "#@markdown El codificador de texto ayuda al Lora a aprender conceptos un poco mejor.  Se recomienda hacerlo la mitad o una quinta parte del unet.  Si est√°s entrenando un estilo, puedes incluso configurarlo en 0.\n",
    "unet_lr = 1e-4 #@param {type:\"number\"}\n",
    "text_encoder_lr = 5e-5 #@param {type:\"number\"}\n",
    "#@markdown El scheduler es el algoritmo que gu√≠a la tasa de aprendizaje. Si no est√° seguro, elije \"constant\" e ignore el n√∫mero. Personalmente recomiendo `cosine_with_restarts` con 3 reinicios.\n",
    "lr_scheduler = \"constant_with_warmup\" # @param [\"constant\",\"cosine\",\"cosine_with_restarts\",\"constant_with_warmup\",\"linear\",\"polynomial\",\"rex\"]\n",
    "lr_scheduler_number = 0 #@param {type:\"number\"}\n",
    "#@markdown Pasos dedicados a \"calentar\" la tasa de aprendizaje durante la capacitaci√≥n para lograr eficiencia. Recomiendo dejarlo al 5%.\n",
    "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.2, step:0.01}\n",
    "lr_warmup_steps = 100 #@param {type:\"number\"}\n",
    "#@markdown Estas configuraciones pueden producir mejores resultados.`min_snr_gamma` ajusta la p√©rdida con el tiempo. `ip_noise_gamma` ajusta el ruido aleatorio.\n",
    "min_snr_gamma_enabled = True #@param {type:\"boolean\"}\n",
    "min_snr_gamma = 8.0 #@param {type:\"slider\", min:4, max:16.0, step:0.5}\n",
    "ip_noise_gamma_enabled = True #@param {type:\"boolean\"}\n",
    "ip_noise_gamma = 0.05 #@param {type:\"slider\", min:0.05, max:0.1, step:0.01}\n",
    "#@markdown Multinoise puede ayudar con el equilibrio del color (negros m√°s oscuros, blancos m√°s claros) no es necesario activarlo si entrenas Lora Vpred.\n",
    "multinoise = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Structure\n",
    "#@markdown LoRA es del tipo cl√°sico y bueno para una variedad de prop√≥sitos. LoCon es bueno con los estilos art√≠sticos (tambi√©n funciona con personajes) ya que tiene m√°s capas para aprender m√°s aspectos del conjunto de datos.\n",
    "lora_type = \"LoRA\" # @param [\"LoRA\",\"LoCon\"]\n",
    "\n",
    "#@markdown A continuaci√≥n se muestran algunos valores XL recomendados para las siguientes configuraciones:\n",
    "\n",
    "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
    "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
    "#@markdown | Personaje LoRA | 4 | 16 |   |   |\n",
    "#@markdown | Regular y Estilo LoRA | 8 | 4 |   |   |\n",
    "#@markdown | Style LoCon | 16 | 8 | 16 | 8 |\n",
    "\n",
    "#@markdown M√°s dim significa un Lora m√°s grande, puede contener m√°s informaci√≥n, pero m√°s no siempre es mejor.\n",
    "network_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "network_alpha = 32 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "#@markdown Los siguientes dos valores solo se aplican a las capas adicionales de LoCon.\n",
    "conv_dim = 16 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "conv_alpha = 8 #@param {type:\"number\", min:1, max:32, step:1}\n",
    "\n",
    "network_module = \"networks.lora\"\n",
    "network_args = None\n",
    "if lora_type.lower() == \"locon\":\n",
    "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Training\n",
    "#@markdown Ajuste estos par√°metros seg√∫n la configuraci√≥n de su colab.\n",
    "\n",
    "#@markdown El batch size de 4 es el predeterminado pero puedes incrementarlo incluso a 8 usando una resoluci√≥n baja (768).\n",
    "#@markdown\n",
    "#@markdown Un tama√±o de lote m√°s alto suele ser m√°s r√°pido pero utiliza m√°s memoria.\n",
    "train_batch_size = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
    "#@markdown xformers funciona mejor que sdpa con los nuevos scrips.\n",
    "cross_attention = \"xformers\" #@param [\"sdpa\", \"xformers\"]\n",
    "#@markdown Utilice `full fp16` para el uso m√≠nimo de memoria. <p>\n",
    "#@markdown `float, full bf16, full fp16, mixed bf16 y mixed fp16` solo funcionaran con colab pro. <p>\n",
    "#@markdown El Lora se entrenar√° con la precisi√≥n seleccionada, pero siempre se guardar√° en formato fp16 por razones de compatibilidad.\n",
    "precision = \"fp16\" #@param [\"float\", \"full fp16\", \"full bf16\", \"mixed fp16\", \"mixed bf16\"]\n",
    "#@markdown El almacenamiento en cach√© latente en disco agregar√° un archivo de 250 KB junto a cada imagen, pero usar√° considerablemente menos memoria.\n",
    "cache_latents = True #@param {type:\"boolean\"}\n",
    "cache_latents_to_disk = False #@param {type:\"boolean\"}\n",
    "#@markdown La siguiente opci√≥n desactivar√° shuffle_tags y deshabilitar√° el entrenamiento del codificador de texto.\n",
    "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
    "\n",
    "mixed_precision = \"no\"\n",
    "if \"fp16\" in precision:\n",
    "  mixed_precision = \"fp16\"\n",
    "elif \"bf16\" in precision:\n",
    "  mixed_precision = \"bf16\"\n",
    "full_precision = \"full\" in precision\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Advanced\n",
    "#@markdown El optimizador es el algoritmo utilizado para el entrenamiento. Adafactor es el predeterminado y funciona muy bien, mientras que el Prodigy administra la tasa de aprendizaje autom√°ticamente y puede tener varias ventajas, como entrenar m√°s r√°pido, debido a que necesita menos pasos y funcionan mejor para datasets peque√±os.\n",
    "optimizer = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\", \"Came\"]\n",
    "#@markdown Argumentos recomendados para Adafactor: `scale_parameter=False relative_step=False warmup_init=False` <p>\n",
    "#@markdown Argumentos recomendados para AdamW8bit: `weight_decay=0.1 betas=[0.9,0.99]`<p>\n",
    "#@markdown Argumentos recomendados para Prodigy: `decouple=True weight_decay=0.01 betas=[0.9,0.999] d_coef=2 use_bias_correction=True safeguard_warmup=True`<p>\n",
    "#@markdown Argumentos recomendado para CAME: `weight_decay=0.04` <p>\n",
    "#@markdown Si se selecciona Dadapt o Prodigy y se marca la casilla recomendada, los siguientes valores recomendados anular√°n cualquier configuraci√≥n anterior:<p>\n",
    "#@markdown `unet_lr=0.75`, `text_encoder_lr=0.75`, `network_alpha=network_dim`, `full_precision=True`<p>\n",
    "#@markdown Si selecciona Prodigy o Dadapt recomiendo usar `mixed fp16`para mejores resultados. <p>\n",
    "recommended_values = True #@param {type:\"boolean\"}\n",
    "#@markdown Alternativamente, establezca sus propios argumentos de optimizador separados por espacios (no comas). `recommended_values` debe estar deshabilitado.\n",
    "optimizer_args = \"\" #@param {type:\"string\"}\n",
    "optimizer_args = [a.strip() for a in optimizer_args.split(' ') if a]\n",
    "\n",
    "\n",
    "if recommended_values:\n",
    "  if any(opt in optimizer.lower() for opt in [\"dadapt\", \"prodigy\"]):\n",
    "    unet_lr = 0.75\n",
    "    text_encoder_lr = 0.75\n",
    "    network_alpha = network_dim\n",
    "    full_precision = False\n",
    "  if optimizer == \"Prodigy\":\n",
    "    optimizer_args = [\"decouple=True\", \"weight_decay=0.01\", \"betas=[0.9,0.999]\", \"d_coef=2\", \"use_bias_correction=True\", \"safeguard_warmup=True\"]\n",
    "  elif optimizer == \"AdamW8bit\":\n",
    "    optimizer_args = [\"weight_decay=0.1\", \"betas=[0.9,0.99]\"]\n",
    "  elif optimizer == \"AdaFactor\":\n",
    "    optimizer_args = [\"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\"]\n",
    "  elif optimizer == \"Came\":\n",
    "    optimizer_args = [\"weight_decay=0.04\"]\n",
    "\n",
    "if optimizer == \"Came\":\n",
    "  optimizer = \"LoraEasyCustomOptimizer.came.CAME\"\n",
    "\n",
    "lr_scheduler_type = None\n",
    "lr_scheduler_args = None\n",
    "lr_scheduler_num_cycles = lr_scheduler_number\n",
    "lr_scheduler_power = lr_scheduler_number\n",
    "\n",
    "if \"rex\" in lr_scheduler:\n",
    "  lr_scheduler = \"cosine\"\n",
    "  lr_scheduler_type = \"LoraEasyCustomOptimizer.RexAnnealingWarmRestarts.RexAnnealingWarmRestarts\"\n",
    "  lr_scheduler_args = [\"min_lr=1e-9\", \"gamma=0.9\", \"d=0.9\"]\n",
    "\n",
    "# Misc\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 1\n",
    "bucket_reso_steps = 64\n",
    "min_bucket_reso = 256\n",
    "max_bucket_reso = 4096\n",
    "\n",
    "#@markdown ### ‚ñ∂Ô∏è Ready\n",
    "#@markdown Ahora puedes ejecutar esta celda para entrenar tu Lora. ¬°Buena suerte! <p>\n",
    "\n",
    "# üë©‚Äçüíª Cool code goes here\n",
    "\n",
    "\n",
    "for required_dir in (models_dir, downloads_dir):\n",
    "  os.makedirs(required_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def lightning_rel(path):\n",
    "  try:\n",
    "    return os.path.relpath(path, root_dir)\n",
    "  except ValueError:\n",
    "    return path\n",
    "\n",
    "\n",
    "venv_python = \"/home/zeus/miniconda3/envs/cloudspace/bin/python3\"\n",
    "#venv_pip = os.path.join(kohya_dir, \"venv/bin/pip\")\n",
    "train_network = os.path.join(kohya_dir, \"sdxl_train_network.py\")\n",
    "\n",
    "if \"lora_projects\" in folder_structure:\n",
    "  main_dir      = os.path.join(root_dir, \"lora_projects\")\n",
    "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
    "  config_folder = os.path.join(main_dir, project_name)\n",
    "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
    "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
    "else:\n",
    "  main_dir      = os.path.join(root_dir, \"lora_training\")\n",
    "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
    "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
    "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
    "  log_folder    = os.path.join(main_dir, \"log\")\n",
    "\n",
    "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
    "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
    "\n",
    "def install_trainer():\n",
    "  global installed\n",
    "  libtcmalloc_path = os.path.join(root_dir, \"libtcmalloc_minimal.so.4\")\n",
    "\n",
    "  if 'installed' not in globals():\n",
    "    installed = False\n",
    "\n",
    "  if not os.path.exists(libtcmalloc_path):\n",
    "    !wget -q -c --show-progress https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O {libtcmalloc_path}\n",
    "\n",
    "  if not os.path.exists(trainer_dir):\n",
    "    !git clone -b dev https://github.com/gwhitez/LoRA_Easy_Training_scripts_Backend.git {trainer_dir}\n",
    "  else:\n",
    "    os.chdir(trainer_dir)\n",
    "    !git pull\n",
    "    os.chdir(root_dir)\n",
    "\n",
    "  os.chdir(trainer_dir)\n",
    "  display(HTML(\"<h2 style='color: yellow;'>Descargando dependencias</h2>\"))\n",
    "  !chmod 755 /teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend/colab_install.sh\n",
    "  !/teamspace/studios/this_studio/LoRA_Easy_Training_scripts_Backend/colab_install.sh > install_log.txt 2>&1\n",
    "\n",
    "  os.chdir(kohya_dir)\n",
    "  if LOAD_TRUNCATED_IMAGES:\n",
    "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py\n",
    "  if BETTER_EPOCH_NAMES:\n",
    "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py\n",
    "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py\n",
    "  if FIX_DIFFUSERS:\n",
    "    deprecation_utils = os.path.join(kohya_dir, \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/diffusers/utils/deprecation_utils.py\")\n",
    "    !sed -i 's/if version.parse/if False:#/g' {deprecation_utils}\n",
    "  if FIX_WANDB_WARNING:\n",
    "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' train_network.py\n",
    "    !sed -i 's/accelerator.log(logs, step=epoch + 1)//g' sdxl_train.py\n",
    "\n",
    "  os.environ[\"LD_PRELOAD\"] = libtcmalloc_path\n",
    "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "  os.chdir(root_dir)\n",
    "\n",
    "def validate_dataset():\n",
    "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, model_url\n",
    "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
    "\n",
    "  print(\"\\nüíø Checking dataset...\")\n",
    "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
    "    print(\"üí• Error: Elija un nombre de proyecto v√°lido.\")\n",
    "    return\n",
    "\n",
    "  # Find the folders and files\n",
    "  if custom_dataset:\n",
    "    try:\n",
    "      datconf = toml.loads(custom_dataset)\n",
    "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
    "    except:\n",
    "      print(f\"üí• Error: El conjunto de datos personalizado no es v√°lido o contiene un error. Por favor, compruebe la plantilla original.\")\n",
    "      return\n",
    "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
    "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
    "    folders = datasets_dict.keys()\n",
    "    files = [f for folder in folders for f in os.listdir(folder)]\n",
    "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
    "  else:\n",
    "    reg = []\n",
    "    folders = [images_folder]\n",
    "    files = os.listdir(images_folder)\n",
    "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
    "\n",
    "  # Validation\n",
    "  for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "      print(f\"üí• Error: La carpeta {lightning_rel(folder)} no existe.\")\n",
    "      return\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    if not img:\n",
    "      print(f\"üí• Error: t√∫ {lightning_rel(folder)} La carpeta est√° vac√≠a.\")\n",
    "      return\n",
    "  test_files = []\n",
    "  for f in files:\n",
    "    if not f.lower().endswith((caption_extension, \".npz\")) and not f.lower().endswith(supported_types):\n",
    "      print(f\"üí• Error: Archivo no v√°lido en el conjunto de datos: \\\"{f}\\\". Abortar.\")\n",
    "      return\n",
    "    for ff in test_files:\n",
    "      if f.endswith(supported_types) and ff.endswith(supported_types) \\\n",
    "          and os.path.splitext(f)[0] == os.path.splitext(ff)[0]:\n",
    "        print(f\"üí• Error: Los archivos {f} y {ff} no puede tener el mismo nombre. Abortar.\")\n",
    "        return\n",
    "    test_files.append(f)\n",
    "\n",
    "  if caption_extension and not [txt for txt in files if txt.lower().endswith(caption_extension)]:\n",
    "    caption_extension = \"\"\n",
    "\n",
    "  # Show estimations to the user\n",
    "\n",
    "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
    "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
    "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
    "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
    "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
    "\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    print(\"üìÅ\" + lightning_rel(folder) + (\" (Regularization)\" if folder in reg else \"\"))\n",
    "    print(f\"üìà Se encontr√≥ {img} im√°genes con {rep} repeticiones, igual {img*rep} pasos.\")\n",
    "  print(f\"üìâ Divide {pre_steps_per_epoch} pasos por {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
    "  if max_train_epochs:\n",
    "    print(f\"üîÆ Habr√° {max_train_epochs} epochs, por alrededor de {total_steps} total de pasos.\")\n",
    "  else:\n",
    "    print(f\"üîÆ Habr√° {total_steps} pasos, divididos en {estimated_epochs} epochs y algo m√°s.\")\n",
    "\n",
    "  if total_steps > 10000:\n",
    "    print(\"üí• Error: El total de pasos es demasiado alto. Probablemente cometiste un error. Abortar...\")\n",
    "    return\n",
    "\n",
    "  return True\n",
    "\n",
    "def create_config():\n",
    "  global dataset_config_file, config_file, model_file\n",
    "\n",
    "  if override_config_file:\n",
    "    config_file = override_config_file\n",
    "    print(f\"\\n‚≠ï Using custom config file {config_file}\")\n",
    "  else:\n",
    "    config_dict = {\n",
    "      \"network_arguments\": {\n",
    "        \"unet_lr\": unet_lr,\n",
    "        \"text_encoder_lr\": text_encoder_lr if not cache_text_encoder_outputs else 0,\n",
    "        \"network_dim\": network_dim,\n",
    "        \"network_alpha\": network_alpha,\n",
    "        \"network_module\": network_module,\n",
    "        \"network_args\": network_args,\n",
    "        \"network_train_unet_only\": text_encoder_lr == 0 or cache_text_encoder_outputs,\n",
    "      },\n",
    "      \"optimizer_arguments\": {\n",
    "        \"learning_rate\": unet_lr,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "        \"lr_scheduler_type\": lr_scheduler_type,\n",
    "        \"lr_scheduler_args\": lr_scheduler_args,\n",
    "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
    "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
    "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler not in (\"cosine\", \"constant\") else None,\n",
    "        \"optimizer_type\": optimizer,\n",
    "       \"optimizer_args\": optimizer_args or None,\n",
    "        \"loss_type\": \"l2\",\n",
    "        \"max_grad_norm\": 1.0,\n",
    "      },\n",
    "      \"training_arguments\": {\n",
    "        \"lowram\": LOWRAM,\n",
    "        \"pretrained_model_name_or_path\": model_file,\n",
    "        \"vae\": vae_file,\n",
    "        \"max_train_steps\": max_train_steps,\n",
    "        \"max_train_epochs\": max_train_epochs,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"seed\": seed,\n",
    "        \"max_token_length\": 225,\n",
    "        \"xformers\": cross_attention == \"xformers\",\n",
    "        \"sdpa\": cross_attention == \"sdpa\",\n",
    "        \"min_snr_gamma\": min_snr_gamma if min_snr_gamma_enabled else None,\n",
    "        \"ip_noise_gamma\": ip_noise_gamma if ip_noise_gamma_enabled else None,\n",
    "        \"no_half_vae\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"max_data_loader_n_workers\": 1,\n",
    "        \"persistent_data_loader_workers\": True,\n",
    "        \"mixed_precision\": mixed_precision,\n",
    "        \"full_fp16\": mixed_precision == \"fp16\" and full_precision,\n",
    "        \"full_bf16\": mixed_precision == \"bf16\" and full_precision,\n",
    "        \"cache_latents\": cache_latents,\n",
    "        \"cache_latents_to_disk\": cache_latents_to_disk,\n",
    "        \"cache_text_encoder_outputs\": cache_text_encoder_outputs,\n",
    "        \"min_timestep\": 0,\n",
    "        \"max_timestep\": 1000,\n",
    "        \"prior_loss_weight\": 1.0,\n",
    "        \"multires_noise_iterations\": 6 if multinoise else None,\n",
    "        \"multires_noise_discount\": 0.3 if multinoise else None,\n",
    "        \"v_parameterization\": vpred or None,\n",
    "        \"scale_v_pred_loss_like_noise_pred\": vpred or None,\n",
    "        \"zero_terminal_snr\": vpred or None,\n",
    "      },\n",
    "      \"saving_arguments\": {\n",
    "        \"save_precision\": \"fp16\",\n",
    "        \"save_model_as\": \"safetensors\",\n",
    "        \"save_every_n_epochs\": save_every_n_epochs,\n",
    "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
    "        \"output_name\": project_name,\n",
    "        \"output_dir\": output_folder,\n",
    "        \"log_prefix\": project_name,\n",
    "        \"logging_dir\": log_folder,\n",
    "        \"wandb_api_key\": wandb_key or None,\n",
    "        \"log_with\": \"wandb\" if wandb_key else None,\n",
    "      }\n",
    "    }\n",
    "\n",
    "    for key in config_dict:\n",
    "      if isinstance(config_dict[key], dict):\n",
    "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(config_dict))\n",
    "    print(f\"\\nüìÑ Config saved to {config_file}\")\n",
    "\n",
    "  if override_dataset_config_file:\n",
    "    dataset_config_file = override_dataset_config_file\n",
    "    print(f\"‚≠ï Using custom dataset config file {dataset_config_file}\")\n",
    "  else:\n",
    "    dataset_config_dict = {\n",
    "      \"general\": {\n",
    "        \"resolution\": resolution,\n",
    "        \"shuffle_caption\": shuffle_caption and not cache_text_encoder_outputs,\n",
    "        \"keep_tokens\": keep_tokens,\n",
    "        \"flip_aug\": False,\n",
    "        \"caption_extension\": caption_extension,\n",
    "        \"enable_bucket\": True,\n",
    "        \"bucket_no_upscale\": False,\n",
    "        \"bucket_reso_steps\": bucket_reso_steps,\n",
    "        \"min_bucket_reso\": min_bucket_reso,\n",
    "        \"max_bucket_reso\": max_bucket_reso,\n",
    "      },\n",
    "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
    "        {\n",
    "          \"subsets\": [\n",
    "            {\n",
    "              \"num_repeats\": num_repeats,\n",
    "              \"image_dir\": images_folder,\n",
    "              \"class_tokens\": None if caption_extension else project_name\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    for key in dataset_config_dict:\n",
    "      if isinstance(dataset_config_dict[key], dict):\n",
    "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(dataset_config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(dataset_config_dict))\n",
    "    print(f\"üìÑ Configuraci√≥n de dataset guardada en {dataset_config_file}\")\n",
    "\n",
    "def download_model():\n",
    "  global old_model_url, model_url, model_file, vae_url, vae_file\n",
    "\n",
    "  real_model_url = (model_url or \"\").strip()\n",
    "  if not real_model_url:\n",
    "    print(\"üí• Error: no se especific√≥ ning√∫n modelo base para entrenar.\")\n",
    "    return False\n",
    "\n",
    "  if load_diffusers:\n",
    "    if 'huggingface.co' in real_model_url:\n",
    "      match = re.search(r'huggingface.co/([^/]+)/([^/]+)', real_model_url)\n",
    "      if match:\n",
    "        username = match.group(1)\n",
    "        model_name = match.group(2)\n",
    "        model_file = f\"{username}/{model_name}\"\n",
    "        from huggingface_hub import HfFileSystem\n",
    "        fs = HfFileSystem()\n",
    "        existing_folders = set(fs.ls(model_file, detail=False))\n",
    "        necessary_folders = [\"scheduler\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"unet\", \"vae\"]\n",
    "        if all(f\"{model_file}/{folder}\" in existing_folders for folder in necessary_folders):\n",
    "          print(\"üçÉ Modelo diffusers identificado; kohya manejar√° la descarga.\")\n",
    "          return True\n",
    "    raise ValueError(\"üí• Failed to load Diffusers model. Si este modelo no es diffusers, desactiva la opci√≥n correspondiente.\")\n",
    "\n",
    "  local_candidate = None\n",
    "  if '://' not in real_model_url:\n",
    "    candidate = Path(real_model_url)\n",
    "    if not candidate.is_absolute():\n",
    "      candidate = Path(root_dir) / real_model_url.lstrip('/')\n",
    "    if candidate.exists():\n",
    "      local_candidate = candidate\n",
    "    else:\n",
    "      print(f\"üí• Error: el modelo local {candidate} no existe. Aseg√∫rate de que est√© dentro de {root_dir} o usa una URL.\")\n",
    "      return False\n",
    "\n",
    "  if local_candidate is not None:\n",
    "    model_file = str(local_candidate)\n",
    "    print(f\"üìÅ Usando modelo local: {model_file}\")\n",
    "  else:\n",
    "    if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
    "      filename = os.path.basename(real_model_url)\n",
    "    else:\n",
    "      filename = \"downloaded_model.safetensors\"\n",
    "\n",
    "    civitai_match = re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", real_model_url)\n",
    "    if civitai_match:\n",
    "      name_hint = civitai_match.group(2)\n",
    "      if name_hint:\n",
    "        filename = f\"{Path(name_hint).name}.safetensors\"\n",
    "      version_match = re.search(r\"modelVersionId=([0-9]+)\", real_model_url)\n",
    "      if version_match:\n",
    "        real_model_url = f\"https://civitai.com/api/download/models/{version_match.group(1)}\"\n",
    "      else:\n",
    "        raise ValueError(\"üí• optional_custom_training_model contiene un enlace de Civitai sin modelVersionId v√°lido.\")\n",
    "\n",
    "    model_file = os.path.join(models_dir, filename)\n",
    "    if os.path.exists(model_file):\n",
    "      !rm \"{model_file}\"\n",
    "\n",
    "    if re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", real_model_url):\n",
    "      real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
    "\n",
    "    print(f\"üåê Descargando modelo en {model_file} ...\")\n",
    "    !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d {models_dir} -o \"{os.path.basename(model_file)}\"\n",
    "\n",
    "    if not os.path.exists(vae_file):\n",
    "      print(f\"üåê Descargando VAE en {vae_file} ...\")\n",
    "      !aria2c \"{vae_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d {models_dir} -o \"{os.path.basename(vae_file)}\"\n",
    "\n",
    "  if model_file.lower().endswith(\".safetensors\"):\n",
    "    from safetensors.torch import load_file as load_safetensors\n",
    "    try:\n",
    "      test = load_safetensors(model_file)\n",
    "      del test\n",
    "    except Exception:\n",
    "      new_model_file = os.path.splitext(model_file)[0] + \".ckpt\"\n",
    "      !mv \"{model_file}\" \"{new_model_file}\"\n",
    "      model_file = new_model_file\n",
    "      print(f\"Renombrado modelo a {model_file}\")\n",
    "\n",
    "  if model_file.lower().endswith(\".ckpt\"):\n",
    "    from torch import load as load_ckpt\n",
    "    try:\n",
    "      test = load_ckpt(model_file)\n",
    "      del test\n",
    "    except Exception:\n",
    "      return False\n",
    "\n",
    "  return True\n",
    "\n",
    "\n",
    "def calculate_rex_steps():\n",
    "  # https://github.com/derrian-distro/LoRA_Easy_Training_scripts_Backend/blob/c34084b0435e6e19bb7a01ac1ecbadd185ee8c1e/utils/validation.py#L268\n",
    "  global max_train_steps\n",
    "  print(\"\\nü§î Calculating Rex steps\")\n",
    "  if max_train_steps:\n",
    "    calculated_max_steps = max_train_steps\n",
    "  else:\n",
    "    from library.train_util import BucketManager\n",
    "    from PIL import Image\n",
    "    from pathlib import Path\n",
    "    import math\n",
    "\n",
    "    with open(dataset_config_file, \"r\") as f:\n",
    "      subsets = toml.load(f)[\"datasets\"][0][\"subsets\"]\n",
    "\n",
    "    supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"]\n",
    "    res = (resolution, resolution)\n",
    "    bucketManager = BucketManager(False, res, min_bucket_reso, max_bucket_reso, bucket_reso_steps)\n",
    "    bucketManager.make_buckets()\n",
    "    for subset in subsets:\n",
    "        for image in Path(subset[\"image_dir\"]).iterdir():\n",
    "            if image.suffix not in supported_types:\n",
    "                continue\n",
    "            with Image.open(image) as img:\n",
    "                bucket_reso, _, _ = bucketManager.select_bucket(img.width, img.height)\n",
    "                for _ in range(subset[\"num_repeats\"]):\n",
    "                    bucketManager.add_image(bucket_reso, image)\n",
    "    steps_before_acc = sum(math.ceil(len(bucket) / train_batch_size) for bucket in bucketManager.buckets)\n",
    "    calculated_max_steps = math.ceil(steps_before_acc / gradient_accumulation_steps) * max_train_epochs\n",
    "    del bucketManager\n",
    "\n",
    "  cycle_steps = calculated_max_steps // (lr_scheduler_num_cycles or 1)\n",
    "  print(f\"  cycle steps: {cycle_steps}\")\n",
    "  lr_scheduler_args.append(f\"first_cycle_max_steps={cycle_steps}\")\n",
    "\n",
    "  warmup_steps = round(calculated_max_steps * lr_warmup_ratio) // (lr_scheduler_num_cycles or 1)\n",
    "  if warmup_steps > 0:\n",
    "    print(f\"  warmup steps: {warmup_steps}\")\n",
    "    lr_scheduler_args.append(f\"warmup_steps={warmup_steps}\")\n",
    "\n",
    "def main():\n",
    "  global dependencies_installed\n",
    "\n",
    "  for dir in (main_dir, trainer_dir, log_folder, images_folder, output_folder, config_folder, models_dir, downloads_dir):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "  if not validate_dataset():\n",
    "    return\n",
    "\n",
    "  if not dependencies_installed:\n",
    "    print(\"üè≠ Instalando entrenador...\")\n",
    "    t0 = time.time()\n",
    "    install_trainer()\n",
    "    t1 = time.time()\n",
    "    dependencies_installed = True\n",
    "    print(f\"‚úÖ Instalaci√≥n terminada en {int(t1 - t0)} segundos.\")\n",
    "  else:\n",
    "    print(\"‚úÖ Dependencias ya instaladas.\")\n",
    "\n",
    "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
    "    print(\"üîÑ Obteniendo modelo...\")\n",
    "    if not download_model():\n",
    "      print(\"üí• Error: el modelo que especific√≥ no es v√°lido o est√° corrupto. Verifique que la URL sea accesible o que la ruta exista dentro de su espacio Lightning.\")\n",
    "      return\n",
    "    print()\n",
    "  else:\n",
    "    print(\"üîÑ Modelo ya disponible.\")\n",
    "\n",
    "  if lr_scheduler_type:\n",
    "    create_config()\n",
    "    os.chdir(kohya_dir)\n",
    "    calculate_rex_steps()\n",
    "    os.chdir(root_dir)\n",
    "\n",
    "  create_config()\n",
    "\n",
    "  print(\"‚≠ê Iniciando Entrenador..\")\n",
    "\n",
    "  os.chdir(kohya_dir)\n",
    "  !{venv_python} {train_network} --config_file={config_file} --dataset_config={dataset_config_file}\n",
    "  os.chdir(root_dir)\n",
    "\n",
    "  if not get_ipython().__dict__.get('user_ns', {}).get('_exit_code', False):\n",
    "    display(Markdown(f\"### ‚úÖ ¬°Hecho! Tus archivos se encuentran en `{output_folder}`\"))\n",
    "\n",
    "main()\n",
    "print(\"üîµ El cuaderno continuar√° en ejecuci√≥n. Det√©n manualmente la sesi√≥n de Lightning cuando termines.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zci9GW_i831B"
   },
   "source": [
    "## Opcional para no perder creditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimina los modelos base antes de cerrar lightning para que no te cobren mas creditos por almacenamiento.\n",
    "!rm -rf models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zci9GW_i831B"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODpo0kcX3KRy"
   },
   "source": [
    "### üìö Varias carpetas en el mismo conjunto de datos\n",
    "A continuaci√≥n se muestra una plantilla que le permite definir varias carpetas en su conjunto de datos. Debe incluir la ubicaci√≥n de cada carpeta y puede establecer un n√∫mero diferente de repeticiones para cada una. Para agregar m√°s carpetas, simplemente copie y pegue las secciones que comienzan con `[[datasets.subsets]]`.\n",
    "\n",
    "Al habilitar esto, se ignorar√° el n√∫mero de repeticiones establecidas en la celda principal y tambi√©n se ignorar√° la carpeta principal establecida por el nombre del proyecto.\n",
    "\n",
    "Puede convertir uno de ellos en una carpeta de regularizaci√≥n agregando `is_reg = true`  \n",
    "Tambi√©n puede establecer diferentes `keep_tokens`, `flip_aug`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y037lagnJWmn"
   },
   "outputs": [],
   "source": [
    "custom_dataset = \"\"\"\n",
    "[[datasets]]\n",
    "\n",
    "[[datasets.subsets]]\n",
    "image_dir = \"/teamspace/studios/this_studio/lora_projects/example/dataset/good_images\"\n",
    "num_repeats = 3\n",
    "\n",
    "[[datasets.subsets]]\n",
    "image_dir = \"/teamspace/studios/this_studio/lora_projects/example/dataset/normal_images\"\n",
    "num_repeats = 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "23_YDYGhEnK0"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Directorio base en Lightning.ai\n",
    "from pathlib import Path\n",
    "lightning_root = Path('/teamspace/studios/this_studio')\n",
    "print(f\"Trabajando en: {lightning_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W84Jxf-U2TIU"
   },
   "outputs": [],
   "source": [
    "custom_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ImAtduziVp5h"
   },
   "outputs": [],
   "source": [
    "##Calculador de Repeticiones ‚åõüìù\n",
    "# Calcula el n√∫mero de repeticiones a usar para entrenar tu lora, Recuerda que en `SDXL y Pony` se usa un batch de `4`.\n",
    "# Si usas colab pro calcula tus repeticiones con `8` de batch size\n",
    "# Define las Variables\n",
    "# N√∫mero de im√°genes\n",
    "num_images = 24 # @param{type:\"number\"}\n",
    "# N√∫mero de repeticiones\n",
    "num_repeats = 2 # @param{type:\"number\"}\n",
    "# N√∫mero de epocas\n",
    "num_epochs = 30 # @param{type:\"number\"}\n",
    "# Tama√±o de lote\n",
    "batch_size = 8 # @param{type:\"number\"}\n",
    "\n",
    "# Calcula el resultado\n",
    "resultado = (num_images * num_repeats * num_epochs) / batch_size\n",
    "\n",
    "# Muestra el resultado\n",
    "print(\"\\33[96mEl total de repeticiones es:\\033[0m\", resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1RfwB4UB3M_w"
   },
   "outputs": [],
   "source": [
    "#@markdown ### üìÇ Descomprimir conjunto de datos\n",
    "#@markdown Sube un archivo `.zip` al almacenamiento de Lightning y descompr√≠melo en la carpeta deseada.\n",
    "zip = \"/teamspace/studios/this_studio/my_dataset.zip\" #@param {type:\"string\"}\n",
    "extract_to = \"/teamspace/studios/this_studio/lora_projects/example/dataset\" #@param {type:\"string\"}\n",
    "\n",
    "import os, zipfile\n",
    "\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "with zipfile.ZipFile(zip, 'r') as f:\n",
    "  f.extractall(extract_to)\n",
    "\n",
    "print(\"‚úÖ Archivo extra√≠do\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
